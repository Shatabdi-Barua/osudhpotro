{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940cc473",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:        \n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#     print(previous_height)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     67\u001b[0m     new_height \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn document.body.scrollHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(new_height)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import sys\n",
    "import json \n",
    "import csv\n",
    "# import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "# databse \n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "from sqlalchemy import insert\n",
    "import pymysql\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "engine = create_engine('mysql://root:@localhost/scrape_data')\n",
    "conn = engine.connect()\n",
    "# metadata = db.MetaData()\n",
    "meta = MetaData()\n",
    "products = db.Table('osudhpotro', meta, autoload=True, autoload_with=engine)\n",
    "category = db.Table('osudhpotro_category', meta, autoload=True, autoload_with=engine)\n",
    "##### for product\n",
    "s = products.select()\n",
    "myresult = conn.execute(s)\n",
    "##### for category\n",
    "# c = category.select()\n",
    "# mycategory = conn.execute(c)\n",
    "#database conn end\n",
    "# for x in myresult:\n",
    "#     print(x[2])\n",
    "#     quit()\n",
    "sys.path.insert(0, 'usr/lib/chromium-browser/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('chromedriver',options=options)\n",
    "driver.get(\"https://osudpotro.com/category/buy-over-the-counter-medicine-online-in-dhaka\") #otc\n",
    "r = driver.page_source\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "url_prefix = \"https://osudpotro.com\"\n",
    "cat_link =[]\n",
    "cat_name = []\n",
    "all_data = dict();\n",
    "for all_cat_div in soup.find_all('div', {'class':'all-prod-list-cont'}):\n",
    "    for one_cat in all_cat_div.find_all('div', {'class':'single-prod-cont product-item'}):\n",
    "        for pro_info in one_cat.find_all('div', {'class':'product-info-wrap'}):\n",
    "            for cat_all_link in pro_info.find_all('a', {'class':'img-placeholder nav-link'}):\n",
    "                cat_link.append(url_prefix+cat_all_link['href'])\n",
    "                \n",
    "        for name in one_cat.find_all('label', {'class':'productTitle'}):\n",
    "            cat_name.append(name.text)\n",
    "#             ins = category.insert().values(category_name = cat_name , \n",
    "#                                             category_link = cat_link)\n",
    "#             reslt = conn.execute(ins)\n",
    "# print(cat_name)\n",
    "# print(cat_link)\n",
    "for ct_link in cat_link:\n",
    "#     print('hello')\n",
    "    c = category.select()\n",
    "    mycategory = conn.execute(c)\n",
    "    driver.get(ct_link)\n",
    "    previous_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:        \n",
    "    #     print(previous_height)\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        print(new_height)\n",
    "        if new_height == previous_height:\n",
    "            print('equal')\n",
    "            break   \n",
    "        previous_height = new_height\n",
    "\n",
    "    r = driver.page_source\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    pro_link = []\n",
    "    datas = [];\n",
    "    all_data['category_link'] = ct_link\n",
    "#     url_prefix = \"https://osudpotro.com\"\n",
    "    for dv in soup.find_all('div', {'class':'disease-inner-page container'}):\n",
    "        for name_row in dv.find_all('div', {'class':'disease-details row'}):        \n",
    "            for nm in name_row.find_all('div', {'class':'disease-name-detail col-lg-8'}):       \n",
    "                for ct_nm in nm.find_all('h3'):\n",
    "                    category_name = ct_nm.text\n",
    "        for rw in dv.find_all('div', {'class': 'row'}):\n",
    "            for cl in rw.find_all('div', {'class':'col'}):\n",
    "                for all_prod in cl.find_all('div', {'class':'all-prod-list-cont'}):\n",
    "                    for single_prod in all_prod.find_all('div', {'class':'single-prod-cont product-item'}):\n",
    "                        for pro_info in single_prod.find_all('div', {'class':'product-info-wrap'}):\n",
    "                            for pro_a in pro_info.find_all('a',{'class':'img-placeholder nav-link'}):\n",
    "                                pro_link.append(url_prefix + pro_a['href'])\n",
    "    print(pro_link)\n",
    "#     all_data['category_name'] = category_name\n",
    "#     cat_not_match = 0\n",
    "#     for ct in mycategory:\n",
    "# #         print('cat_in_db:', ct[2])\n",
    "# #         print('cat:',all_data['category_link'])\n",
    "#         if ct[2] ==  all_data['category_link']:\n",
    "#             cat_not_match = 0\n",
    "#             break;\n",
    "#         else:\n",
    "#              cat_not_match += 1\n",
    "#     if cat_not_match != 0:\n",
    "# #         print(all_data['category_link'])\n",
    "#         cat_ins = category.insert().values(category_name = all_data['category_name'], \n",
    "#                                                 category_link = all_data['category_link'])\n",
    "#         reslt = conn.execute(cat_ins)\n",
    "# #     print(pro_link)\n",
    "#     if pro_link == []:\n",
    "#         status = 0 #no data found\n",
    "# #         print('status', status)\n",
    "# #         print(ct_link);\n",
    "# #         for ct_l in myresult:\n",
    "# #             if ct_l[2] == ct_link:\n",
    "# #                 print('cat match')\n",
    "# #                 break\n",
    "# #             else:\n",
    "# #                 ins = products.insert().values(category_name = all_data['category_name'] , \n",
    "# #                                             category_link = all_data['category_link'], status=0 )\n",
    "# #                 reslt = conn.execute(ins)\n",
    "# #         for ct_link in myresult:\n",
    "# #             print(ct_link[2])\n",
    "# #             if ct_link[2] == all_data['category_link']:\n",
    "# #                 continue;\n",
    "# #             else:\n",
    "# #                 ins = products.insert().values(category_name = all_data['category_name'] , \n",
    "# #                                                            category_link = all_data['category_link'], status=0 )\n",
    "# #                 reslt = conn.execute(ins)\n",
    "#     # print(pro_link)\n",
    "#     else:        \n",
    "#         for u in pro_link:\n",
    "#             status = 1\n",
    "#     #         driver.get(pr)\n",
    "#     #         r = driver.page_source\n",
    "#     #         soup = BeautifulSoup(r, 'html.parser')\n",
    "#     #         for u in url: \n",
    "#             print('url', u)\n",
    "#             product_images =[];\n",
    "#             discount_percent = [];\n",
    "#             slider_images = [];\n",
    "#             title_name = [];\n",
    "#             med_type = [];\n",
    "#             a_reference = [];\n",
    "#             a_text = [];\n",
    "#             generic_name = [];\n",
    "#             special_price_name = [];\n",
    "#             special_price_val = [];\n",
    "#             regular_price = [];\n",
    "#             option_name = [];\n",
    "#             product_description = [];\n",
    "#             recommended_products = [];\n",
    "#             disclaimer = [];\n",
    "#             reference_first_part = \"https://osudpotro.com/\";\n",
    "#             recom_img_link = [];       \n",
    "#             all_data['product_url'] = []\n",
    "#             all_data['product_images'] = []\n",
    "#             all_data['img_discount_percent']= []\n",
    "#             all_data['slider_images']= []\n",
    "#             all_data['medicine type']= []\n",
    "#             all_data['medicine name']= []\n",
    "#             all_data['generic_link']  = []\n",
    "#             all_data['generic_name'] = []\n",
    "#             all_data['manufacturer name'] = []\n",
    "#             all_data['manufacturer link'] = []\n",
    "#             all_data['special_price'] = []\n",
    "#             all_data['regular_price'] = []\n",
    "#             all_data['variation_option_name'] = []\n",
    "#             all_data['buy_btn_text']= []\n",
    "#             all_data['cart_btn_txt']= []\n",
    "#             all_data['product_description']= []\n",
    "#             all_data['recommended_products']= []\n",
    "#             all_data['disclaimer']= []\n",
    "#             driver.get(u)\n",
    "#             r = driver.page_source\n",
    "#             soup = BeautifulSoup(r, 'html.parser')\n",
    "#             all_data['product_url'] = u\n",
    "#             for img_div in soup.find_all('div',{'class': 'product-image-cont-compo'}):\n",
    "#             ######################### product_images ###################\n",
    "#                 for im in img_div.find_all('img'):\n",
    "#                     product_images.append(im['src'])\n",
    "#                 all_data['product_images'] = product_images\n",
    "#             ########################## discount_percent ####################\n",
    "#                 for dis in img_div.find_all('div',{'class':'discount-percent'}):\n",
    "#                     discount_percent.append(dis.text)\n",
    "#                 all_data['img_discount_percent'] = discount_percent\n",
    "#             ######################### slider_images ###################\n",
    "#             for sld in soup.find_all('div', {'class': 'image-swipper-cont'}):\n",
    "#                 for sld_img_div in sld.find_all('div', {'class': 'swiper-slide image-swipper swiper-slide-active'}):\n",
    "#                     for sld_im in sld_img_div.find_all('img'):\n",
    "#                         slider_images.append(sld_im['src'])\n",
    "#                     all_data['slider_images'] = slider_images\n",
    "#             for maindiv in soup.find_all('div', {'class': 'product-info-cont-compo product-details-details-inner-page col-lg-8'}):\n",
    "#                     for title_div in maindiv.find_all('div', {'class':'row'}):\n",
    "#                         for title_col in title_div.find_all('div', {'class':'col-lg-9'}):\n",
    "#                         ######################### [h1] medicine name & type ###################\n",
    "#                             for title in title_col.find_all('h1', { 'class' : 'product-info-heading-compo'}):                    \n",
    "#                                 for title_span in title.find_all('span', {'class' : 'sku'}):\n",
    "#                                     med_type.append(title_span.text)\n",
    "#                                     all_data['medicine type'] = med_type\n",
    "#                                 title.find('span').extract()\n",
    "#                                 title_name.append(title.getText().strip())\n",
    "#                             all_data['medicine name'] = title_name\n",
    "#                         ######################### generic name ###################      \n",
    "#                             for generics in title_col.find_all('div', {'class':'generes-wrap'}):\n",
    "#                                 for gn_link in generics.find_all('a', {'class':'nav-link'}):\n",
    "#                                     generic_link = gn_link['href']\n",
    "#                                     for gn_name in gn_link.find_all('span'):\n",
    "#                                         generic_name.append(gn_name.text)\n",
    "#                             all_data['generic_link'] = generic_link\n",
    "#                             all_data['generic_name'] = generic_name;\n",
    "#                         ######################### [p] manufacturer name, link ###################\n",
    "#                             for p_manufacturer in title_col.find_all('p', {'class':'menufacturer'}):\n",
    "#                                 for manf_link in p_manufacturer.find_all('a', {'class' : 'nav-link'}):\n",
    "#                                     a_reference.append(reference_first_part+manf_link['href']);\n",
    "#                                     a_text.append(manf_link.text)\n",
    "#                                 all_data['manufacturer name'] = a_text\n",
    "#                                 all_data['manufacturer link'] = a_reference                        \n",
    "\n",
    "#             data_price = json.loads(soup.find('script', id=\"__NEXT_DATA__\").text)\n",
    "#             inv = data_price['props']['initialProps']['pageProps']['productData']['inventory']\n",
    "#             for prices in inv:\n",
    "#                 special_price_val.append(prices[\"price\"])\n",
    "#                 regular_price.append(prices[\"regular_price\"])\n",
    "#                 option_name.append(prices['variation_option_name'])\n",
    "#             all_data['special_price'] = special_price_val\n",
    "#             all_data['regular_price'] = regular_price\n",
    "#             all_data['variation_option_name'] = option_name\n",
    "#         ######################### buttons ###################\n",
    "#             for buttons in soup.find_all('div',{'class':'prod-control-buttons d-flex justify-content-between'}):\n",
    "#                 for buy_button in buttons.find_all('button', {'class':'btn buy-more-button ml-3 mr-2'}):\n",
    "#                     buy_button.find('i').extract()\n",
    "#                     buy_btn_text = buy_button.text\n",
    "#                 all_data['buy_btn_text'] = buy_btn_text\n",
    "#                 for cart_butn in buttons.find_all('button', { 'class' : 'btn btn-secondary details-checkout-btn ml-2'}):\n",
    "#                     cart_butn.find('i').extract()\n",
    "#                     cart_btn_txt = cart_butn.text            \n",
    "#                 all_data['cart_btn_txt'] = cart_btn_txt\n",
    "#             for pro_des in soup.find_all('div', { 'class' : 'd-none d-sm-block row'}):\n",
    "#                 product_description.append(pro_des)\n",
    "#             all_data['product_description'] = product_description\n",
    "\n",
    "#             for swp in soup.find_all('div', {'class':'jss3'}):\n",
    "#                 for swp_wrap in swp.find_all('div', {'class':'swiper-container jss5 swiper-container-initialized swiper-container-horizontal swiper-container-pointer-events'}):\n",
    "#                     for sld in swp.find_all('div', {'class':'swiper-wrapper'}):\n",
    "#                         all_data['recommended_products'] = sld\n",
    "#             for disc_cont in soup.find_all('div' , { 'class': 'container'}):\n",
    "#                 for disc_row in disc_cont.find_all('div', {'class':'row'}):\n",
    "#                     for disc_col in disc_row.find_all('div', {'class':'col'}):\n",
    "#                         for disc_p in disc_col.find_all('p', {'class':'product-disclaimer'}):\n",
    "#                             disclaimer.append(disc_p.text)   \n",
    "#                         all_data['disclaimer'] = disclaimer\n",
    "#             datas.append(all_data)\n",
    "# #             print(datas)\n",
    "# #             print(myresult)\n",
    "            \n",
    "#             for x in myresult:\n",
    "#                 myresult = conn.execute(s)\n",
    "#                 match = 0 \n",
    "# #                 print('db data: ', x[3])\n",
    "# #                 print('url', all_data['product_url'])\n",
    "# #                 print('ca:',all_data['category_link'])\n",
    "# #                 exit()\n",
    "#                 if x[3] == all_data['product_url']:\n",
    "# #                     al =1;\n",
    "#                     match = 0 \n",
    "# #                     print('product matched');\n",
    "#                     break\n",
    "# #                     continue;\n",
    "#                 else:\n",
    "#                     match += 1\n",
    "# #                     print('not matched')\n",
    "#             if match != 0:\n",
    "#                 print('insert')\n",
    "#                 ins = products.insert().values(category_name = str(all_data['category_name']) , \n",
    "#                                                    category_link = str(all_data['category_link']),\n",
    "#                                                    product_url =str(all_data['product_url']), \n",
    "#                                                    product_images=str(all_data['product_images']),  \n",
    "#                                                    img_discount_percent=str(all_data['img_discount_percent']),\n",
    "#                                                    slider_images=str(all_data['slider_images']),\n",
    "#                                                    medicine_type=str(all_data['medicine type']),\n",
    "#                                                    medicine_name=str(all_data['medicine name']),\n",
    "#                                                    generic_link=str(all_data['generic_link']),\n",
    "#                                                    generic_name=str(all_data['generic_name']), \n",
    "#                                                    manufacturer_name =str(all_data['manufacturer name']),\n",
    "#                                                    manufacturer_link =str(all_data['manufacturer link']), \n",
    "#                                                    special_price=str(all_data['special_price']),\n",
    "#                                                    regular_price=str(all_data['regular_price']),\n",
    "#                                                    variation_option_name=str(all_data['variation_option_name']),\n",
    "#                                                    buy_btn_text=str(all_data['buy_btn_text']), \n",
    "#                                                    product_description = str(all_data['product_description']),\n",
    "#                                                    recommended_products = str(all_data['recommended_products']), \n",
    "#                                                    disclaimer= str(all_data['disclaimer']), status=1 \n",
    "#                                                     )\n",
    "#                 reslt = conn.execute(ins)    \n",
    "# #                     break\n",
    "# #                     print('result', reslt)\n",
    "# #                     ins = products.insert().values(category_name = str(all_data['category_name']) , \n",
    "# #                                                    category_link = str(all_data['category_link']),\n",
    "# #                                                    product_url =str(all_data['product_url']), \n",
    "# #                                                    product_images=str(all_data['product_images']),                                                      \n",
    "# #                                                        img_discount_percent=str(all_data['img_discount_percent']),\n",
    "# #                                                        slider_images=str(all_data['slider_images']),\n",
    "# #                                                        medicine_type=str(all_data['medicine type']), \n",
    "# #                                                        medicine_name=str(all_data['medicine name']),\n",
    "# #                                                        generic_link=str(all_data['generic_link']),\n",
    "# #                                                        generic_name=str(all_data['generic_name']), \n",
    "# #                                                        manufacturer_name =str(all_data['manufacturer name']),\n",
    "# #                                                        manufacturer_link =str(all_data['manufacturer link']), \n",
    "# #                                                        special_price=str(all_data['special_price']),regular_price=str(all_data['regular_price']), \n",
    "# #                                                        variation_option_name=all_data['variation_option_name'], \n",
    "# #                                                        buy_btn_text=all_data['buy_btn_text'], product_description = all_data['product_description'],\n",
    "# #                                                        recommended_products = all_data['recommended_products'], disclaimer=all_data['disclaimer'], status=1 )\n",
    "# # #                     print(ins)\n",
    "# #                     reslt = conn.execute(ins)\n",
    "# #     print(datas)\n",
    "# #     if status == 0:\n",
    "# # #         print('2')\n",
    "# #         for ct_l in myresult:\n",
    "# #             if ct_l[2] == ct_link:\n",
    "# #                 print('cat match')\n",
    "# # #             print(ct_l[2])\n",
    "\n",
    "# #         ins = products.insert().values(category_name = all_data['category_name'] , \n",
    "# #                                                    category_link = all_data['category_link'], status=0 )\n",
    "# #         reslt = conn.execute(ins)\n",
    "driver.close()\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b463a893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Using cached PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.0.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa103dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Using cached SQLAlchemy-1.4.36-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-1.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-1.1.2 sqlalchemy-1.4.36\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "931e86c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.1.5-py3-none-any.whl (979 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.20.0-py3-none-any.whl (359 kB)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in /media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/lib/python3.8/site-packages (from selenium) (1.26.9)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting sortedcontainers\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: idna in /media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/lib/python3.8/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/lib/python3.8/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: certifi in /media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium) (2022.5.18.1)\n",
      "Collecting cryptography>=1.3.4\n",
      "  Using cached cryptography-37.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.0 MB)\n",
      "Collecting pyOpenSSL>=0.14\n",
      "  Using cached pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/lib/python3.8/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Using cached h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pycparser in /media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
      "Installing collected packages: sortedcontainers, sniffio, PySocks, outcome, h11, async-generator, wsproto, trio, cryptography, trio-websocket, pyOpenSSL, selenium\n",
      "Successfully installed PySocks-1.7.1 async-generator-1.10 cryptography-37.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.5 sniffio-1.2.0 sortedcontainers-2.4.0 trio-0.20.0 trio-websocket-0.9.2 wsproto-1.1.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085f864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.5.18.1-py3-none-any.whl (155 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2022.5.18.1 charset-normalizer-2.0.12 idna-3.3 requests-2.27.1 urllib3-1.26.9\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the '/media/trenza/6C50D42C50D3FAB2/Shatabdi/osudhpotro/osudhpotro_data_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93477230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "from sqlalchemy import insert\n",
    "import pymysql\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "engine = create_engine('mysql://root:@localhost/web_data')\n",
    "conn = engine.connect()\n",
    "# metadata = db.MetaData()\n",
    "meta = MetaData()\n",
    "products = db.Table('osudhpotro', meta, autoload=True, autoload_with=engine)\n",
    "s = products.select()\n",
    "myresult = conn.execute(s)\n",
    "for x in myresult:\n",
    "    print(x[2])\n",
    "#                 exit()\n",
    "    if x[2] == all_data['product_url']:\n",
    "        print('products matched', all_data['product_url']);\n",
    "#                     continue;\n",
    "    else:\n",
    "        print('insert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0d48091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status 0\n",
      "url https://osudpotro.com/neocort-ointment\n",
      "insert\n",
      "result <sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7f330c927dc0>\n",
      "url https://osudpotro.com/betson-cream\n",
      "insert\n",
      "result <sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7f330d1e3ac0>\n",
      "url https://osudpotro.com/betson-ointment\n",
      "insert\n",
      "result <sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7f330c91fbb0>\n",
      "url https://osudpotro.com/betameson-cream\n",
      "insert\n",
      "result <sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7f330c927e20>\n",
      "url https://osudpotro.com/prosalic-lotion-25-ml-scalp-lotion\n",
      "insert\n",
      "result <sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7f330c9fa040>\n",
      "url https://osudpotro.com/betamesal-ointment\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecommended_products\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    120\u001b[0m all_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisclaimer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 121\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m r \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[1;32m    123\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(r, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:437\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    Loads a web page in the current browser session.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:423\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    420\u001b[0m         params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m    422\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_value(params)\n\u001b[0;32m--> 423\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:333\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    331\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[1;32m    332\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:355\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    352\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 355\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m resp\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/urllib3/request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[38;5;241m=\u001b[39mfields, headers\u001b[38;5;241m=\u001b[39mheaders, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/urllib3/request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(headers)\n\u001b[1;32m    168\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             \u001b[43msix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/media/trenza/New Volume/Shatabdi/osudhpotro/osudhpotro_env/lib/python3.8/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;66;03m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;66;03m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;66;03m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(e, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import sys\n",
    "import json \n",
    "import csv\n",
    "# import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "# databse \n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String\n",
    "from sqlalchemy import insert\n",
    "import pymysql\n",
    "\n",
    "pymysql.install_as_MySQLdb()\n",
    "engine = create_engine('mysql://root:@localhost/web_data')\n",
    "conn = engine.connect()\n",
    "# metadata = db.MetaData()\n",
    "meta = MetaData()\n",
    "products = db.Table('osudhpotro', meta, autoload=True, autoload_with=engine)\n",
    "s = products.select()\n",
    "myresult = conn.execute(s)\n",
    "#database conn end\n",
    "# for x in myresult:\n",
    "#     print(x[2])\n",
    "#     quit()\n",
    "sys.path.insert(0, 'usr/lib/chromium-browser/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('chromedriver',options=options)\n",
    "driver.get(\"https://osudpotro.com/category/buy-over-the-counter-medicine-online-in-dhaka\")\n",
    "r = driver.page_source\n",
    "soup = BeautifulSoup(r, 'html.parser')\n",
    "url_prefix = \"https://osudpotro.com\"\n",
    "cat_link =[]\n",
    "cat_name = []\n",
    "all_data = dict();\n",
    "for all_cat_div in soup.find_all('div', {'class':'all-prod-list-cont'}):\n",
    "    for one_cat in all_cat_div.find_all('div', {'class':'single-prod-cont product-item'}):\n",
    "        for pro_info in one_cat.find_all('div', {'class':'product-info-wrap'}):\n",
    "            for cat_all_link in pro_info.find_all('a', {'class':'img-placeholder nav-link'}):\n",
    "                cat_link.append(url_prefix+cat_all_link['href'])\n",
    "        for name in one_cat.find_all('label', {'class':'productTitle'}):\n",
    "            cat_name.append(name.text)\n",
    "# print(cat_name)\n",
    "# print(cat_link)\n",
    "for ct_link in cat_link:\n",
    "    \n",
    "    driver.get(ct_link)\n",
    "    r = driver.page_source\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    pro_link = []\n",
    "    datas = [];\n",
    "    all_data['category_link'] = ct_link\n",
    "#     url_prefix = \"https://osudpotro.com\"\n",
    "    for dv in soup.find_all('div', {'class':'disease-inner-page container'}):\n",
    "        for name_row in dv.find_all('div', {'class':'disease-details row'}):        \n",
    "            for nm in name_row.find_all('div', {'class':'disease-name-detail col-lg-8'}):       \n",
    "                for ct_nm in nm.find_all('h3'):\n",
    "                    category_name = ct_nm.text\n",
    "        for rw in dv.find_all('div', {'class': 'row'}):\n",
    "            for cl in rw.find_all('div', {'class':'col'}):\n",
    "                for all_prod in cl.find_all('div', {'class':'all-prod-list-cont'}):\n",
    "                    for single_prod in all_prod.find_all('div', {'class':'single-prod-cont product-item'}):\n",
    "                        for pro_info in single_prod.find_all('div', {'class':'product-info-wrap'}):\n",
    "                            for pro_a in pro_info.find_all('a',{'class':'img-placeholder nav-link'}):\n",
    "                                pro_link.append(url_prefix + pro_a['href'])\n",
    "    all_data['category_name'] = category_name\n",
    "    if pro_link == []:\n",
    "        status = 0 #no data found\n",
    "#         print('status', status)\n",
    "#         ins = products.insert().values(category_name = all_data['category_name'] , \n",
    "#                                                    category_link = all_data['category_link'], status=0 )\n",
    "#         reslt = conn.execute(ins)\n",
    "    # print(pro_link)\n",
    "    else:\n",
    "        \n",
    "        for u in pro_link:\n",
    "            status = 1\n",
    "    #         driver.get(pr)\n",
    "    #         r = driver.page_source\n",
    "    #         soup = BeautifulSoup(r, 'html.parser')\n",
    "    #         for u in url: \n",
    "#             print('url', u)\n",
    "            product_images =[];\n",
    "            discount_percent = [];\n",
    "            slider_images = [];\n",
    "            title_name = [];\n",
    "            med_type = [];\n",
    "            a_reference = [];\n",
    "            a_text = [];\n",
    "            generic_name = [];\n",
    "            special_price_name = [];\n",
    "            special_price_val = [];\n",
    "            regular_price = [];\n",
    "            option_name = [];\n",
    "            product_description = [];\n",
    "            recommended_products = [];\n",
    "            disclaimer = [];\n",
    "            reference_first_part = \"https://osudpotro.com/\";\n",
    "            recom_img_link = [];       \n",
    "            all_data['product_url'] = []\n",
    "            all_data['product_images'] = []\n",
    "            all_data['img_discount_percent']= []\n",
    "            all_data['slider_images']= []\n",
    "            all_data['medicine type']= []\n",
    "            all_data['medicine name']= []\n",
    "            all_data['generic_link']  = []\n",
    "            all_data['generic_name'] = []\n",
    "            all_data['manufacturer name'] = []\n",
    "            all_data['manufacturer link'] = []\n",
    "            all_data['special_price'] = []\n",
    "            all_data['regular_price'] = []\n",
    "            all_data['variation_option_name'] = []\n",
    "            all_data['buy_btn_text']= []\n",
    "            all_data['cart_btn_txt']= []\n",
    "            all_data['product_description']= []\n",
    "            all_data['recommended_products']= []\n",
    "            all_data['disclaimer']= []\n",
    "            driver.get(u)\n",
    "            r = driver.page_source\n",
    "            soup = BeautifulSoup(r, 'html.parser')\n",
    "            all_data['product_url'] = u\n",
    "            for img_div in soup.find_all('div',{'class': 'product-image-cont-compo'}):\n",
    "            ######################### product_images ###################\n",
    "                for im in img_div.find_all('img'):\n",
    "                    product_images.append(im['src'])\n",
    "                all_data['product_images'] = product_images\n",
    "            ########################## discount_percent ####################\n",
    "                for dis in img_div.find_all('div',{'class':'discount-percent'}):\n",
    "                    discount_percent.append(dis.text)\n",
    "                all_data['img_discount_percent'] = discount_percent\n",
    "            ######################### slider_images ###################\n",
    "            for sld in soup.find_all('div', {'class': 'image-swipper-cont'}):\n",
    "                for sld_img_div in sld.find_all('div', {'class': 'swiper-slide image-swipper swiper-slide-active'}):\n",
    "                    for sld_im in sld_img_div.find_all('img'):\n",
    "                        slider_images.append(sld_im['src'])\n",
    "                    all_data['slider_images'] = slider_images\n",
    "            for maindiv in soup.find_all('div', {'class': 'product-info-cont-compo product-details-details-inner-page col-lg-8'}):\n",
    "                    for title_div in maindiv.find_all('div', {'class':'row'}):\n",
    "                        for title_col in title_div.find_all('div', {'class':'col-lg-9'}):\n",
    "                        ######################### [h1] medicine name & type ###################\n",
    "                            for title in title_col.find_all('h1', { 'class' : 'product-info-heading-compo'}):                    \n",
    "                                for title_span in title.find_all('span', {'class' : 'sku'}):\n",
    "                                    med_type.append(title_span.text)\n",
    "                                    all_data['medicine type'] = med_type\n",
    "                                title.find('span').extract()\n",
    "                                title_name.append(title.getText().strip())\n",
    "                            all_data['medicine name'] = title_name\n",
    "                        ######################### generic name ###################      \n",
    "                            for generics in title_col.find_all('div', {'class':'generes-wrap'}):\n",
    "                                for gn_link in generics.find_all('a', {'class':'nav-link'}):\n",
    "                                    generic_link = gn_link['href']\n",
    "                                    for gn_name in gn_link.find_all('span'):\n",
    "                                        generic_name.append(gn_name.text)\n",
    "                            all_data['generic_link'] = generic_link\n",
    "                            all_data['generic_name'] = generic_name;\n",
    "                        ######################### [p] manufacturer name, link ###################\n",
    "                            for p_manufacturer in title_col.find_all('p', {'class':'menufacturer'}):\n",
    "                                for manf_link in p_manufacturer.find_all('a', {'class' : 'nav-link'}):\n",
    "                                    a_reference.append(reference_first_part+manf_link['href']);\n",
    "                                    a_text.append(manf_link.text)\n",
    "                                all_data['manufacturer name'] = a_text\n",
    "                                all_data['manufacturer link'] = a_reference                        \n",
    "\n",
    "            data_price = json.loads(soup.find('script', id=\"__NEXT_DATA__\").text)\n",
    "            inv = data_price['props']['initialProps']['pageProps']['productData']['inventory']\n",
    "            for prices in inv:\n",
    "                special_price_val.append(prices[\"price\"])\n",
    "                regular_price.append(prices[\"regular_price\"])\n",
    "                option_name.append(prices['variation_option_name'])\n",
    "            all_data['special_price'] = special_price_val\n",
    "            all_data['regular_price'] = regular_price\n",
    "            all_data['variation_option_name'] = option_name\n",
    "        ######################### buttons ###################\n",
    "            for buttons in soup.find_all('div',{'class':'prod-control-buttons d-flex justify-content-between'}):\n",
    "                for buy_button in buttons.find_all('button', {'class':'btn buy-more-button ml-3 mr-2'}):\n",
    "                    buy_button.find('i').extract()\n",
    "                    buy_btn_text = buy_button.text\n",
    "                all_data['buy_btn_text'] = buy_btn_text\n",
    "                for cart_butn in buttons.find_all('button', { 'class' : 'btn btn-secondary details-checkout-btn ml-2'}):\n",
    "                    cart_butn.find('i').extract()\n",
    "                    cart_btn_txt = cart_butn.text            \n",
    "                all_data['cart_btn_txt'] = cart_btn_txt\n",
    "            for pro_des in soup.find_all('div', { 'class' : 'd-none d-sm-block row'}):\n",
    "                product_description.append(pro_des)\n",
    "            all_data['product_description'] = product_description\n",
    "\n",
    "            for swp in soup.find_all('div', {'class':'jss3'}):\n",
    "                for swp_wrap in swp.find_all('div', {'class':'swiper-container jss5 swiper-container-initialized swiper-container-horizontal swiper-container-pointer-events'}):\n",
    "                    for sld in swp.find_all('div', {'class':'swiper-wrapper'}):\n",
    "                        all_data['recommended_products'] = sld\n",
    "            for disc_cont in soup.find_all('div' , { 'class': 'container'}):\n",
    "                for disc_row in disc_cont.find_all('div', {'class':'row'}):\n",
    "                    for disc_col in disc_row.find_all('div', {'class':'col'}):\n",
    "                        for disc_p in disc_col.find_all('p', {'class':'product-disclaimer'}):\n",
    "                            disclaimer.append(disc_p.text)   \n",
    "                        all_data['disclaimer'] = disclaimer\n",
    "            datas.append(all_data)\n",
    "#             print(datas)\n",
    "#             print(myresult)\n",
    "#             print('insert')\n",
    "            ins = products.insert().values(category_name = str(all_data['category_name']) , \n",
    "                                                   category_link = str(all_data['category_link']),\n",
    "                                                   product_url =str(all_data['product_url']), \n",
    "                                                   product_images=str(all_data['product_images']),  \n",
    "                                                   img_discount_percent=str(all_data['img_discount_percent']),\n",
    "                                                   slider_images=str(all_data['slider_images']),\n",
    "                                                   medicine_type=str(all_data['medicine type']),\n",
    "                                                   medicine_name=str(all_data['medicine name']),\n",
    "                                                   generic_link=str(all_data['generic_link']),\n",
    "                                                   generic_name=str(all_data['generic_name']), \n",
    "                                                   manufacturer_name =str(all_data['manufacturer name']),\n",
    "                                                   manufacturer_link =str(all_data['manufacturer link']), \n",
    "                                                   special_price=str(all_data['special_price']),\n",
    "                                                   regular_price=str(all_data['regular_price']),\n",
    "                                                   variation_option_name=str(all_data['variation_option_name']),\n",
    "                                                   buy_btn_text=str(all_data['buy_btn_text']), \n",
    "                                                   product_description = str(all_data['product_description']),\n",
    "                                                   recommended_products = str(all_data['recommended_products']), \n",
    "                                                   disclaimer= str(all_data['disclaimer']), status=1 \n",
    "                                                    )\n",
    "            reslt = conn.execute(ins)    \n",
    "#             print('result', reslt)\n",
    "            \n",
    "                    \n",
    "#                     ins = products.insert().values(category_name = str(all_data['category_name']) , \n",
    "#                                                    category_link = str(all_data['category_link']),\n",
    "#                                                    product_url =str(all_data['product_url']), \n",
    "#                                                    product_images=str(all_data['product_images']),                                                      \n",
    "#                                                        img_discount_percent=str(all_data['img_discount_percent']),\n",
    "#                                                        slider_images=str(all_data['slider_images']),\n",
    "#                                                        medicine_type=str(all_data['medicine type']), \n",
    "#                                                        medicine_name=str(all_data['medicine name']),\n",
    "#                                                        generic_link=str(all_data['generic_link']),\n",
    "#                                                        generic_name=str(all_data['generic_name']), \n",
    "#                                                        manufacturer_name =str(all_data['manufacturer name']),\n",
    "#                                                        manufacturer_link =str(all_data['manufacturer link']), \n",
    "#                                                        special_price=str(all_data['special_price']),regular_price=str(all_data['regular_price']), \n",
    "#                                                        variation_option_name=all_data['variation_option_name'], \n",
    "#                                                        buy_btn_text=all_data['buy_btn_text'], product_description = all_data['product_description'],\n",
    "#                                                        recommended_products = all_data['recommended_products'], disclaimer=all_data['disclaimer'], status=1 )\n",
    "#                     print(ins)\n",
    "#                     reslt = conn.execute(ins)\n",
    "#     print(datas)\n",
    "#     if status == 0:\n",
    "#         ins = products.insert().values(category_name = all_data['category_name'] , \n",
    "#                                                    category_link = all_data['category_link'], status=0 )\n",
    "#         reslt = conn.execute(ins)\n",
    "driver.close()\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
