{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "741784c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.3-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
      "Installing collected packages: certifi, urllib3, idna, charset-normalizer, requests\n",
      "Successfully installed certifi-2021.10.8 charset-normalizer-2.0.12 idna-3.3 requests-2.27.1 urllib3-1.26.9\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd435fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Using cached selenium-4.1.3-py3-none-any.whl (968 kB)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in ./osudhpotro_env/lib/python3.8/site-packages (from selenium) (1.26.9)\n",
      "Collecting trio~=0.17\n",
      "  Using cached trio-0.20.0-py3-none-any.whl (359 kB)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Using cached trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
      "Collecting sniffio\n",
      "  Using cached sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting outcome\n",
      "  Using cached outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./osudhpotro_env/lib/python3.8/site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in ./osudhpotro_env/lib/python3.8/site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Collecting sortedcontainers\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting async-generator>=1.9\n",
      "  Using cached async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Collecting wsproto>=0.14\n",
      "  Using cached wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting PySocks!=1.5.7,<2.0,>=1.5.6\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: certifi in ./osudhpotro_env/lib/python3.8/site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Collecting cryptography>=1.3.4\n",
      "  Using cached cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
      "Collecting pyOpenSSL>=0.14\n",
      "  Using cached pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in ./osudhpotro_env/lib/python3.8/site-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Using cached h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pycparser in ./osudhpotro_env/lib/python3.8/site-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
      "Installing collected packages: sortedcontainers, sniffio, PySocks, outcome, h11, async-generator, wsproto, trio, cryptography, trio-websocket, pyOpenSSL, selenium\n",
      "Successfully installed PySocks-1.7.1 async-generator-1.10 cryptography-36.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.3 sniffio-1.2.0 sortedcontainers-2.4.0 trio-0.20.0 trio-websocket-0.9.2 wsproto-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d676648b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu0.20.04.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 19 not upgraded.\n",
      "yes: standard output: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!yes|apt install chromium-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9abf0931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in ./osudhpotro_env/lib/python3.8/site-packages (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in ./osudhpotro_env/lib/python3.8/site-packages (from BeautifulSoup4) (2.3.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4eca88cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.0.9-py2.py3-none-any.whl (242 kB)\n",
      "Collecting et-xmlfile\n",
      "  Using cached et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f252eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "Collecting numpy>=1.18.5\n",
      "  Using cached numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./osudhpotro_env/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./osudhpotro_env/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "Successfully installed numpy-1.22.3 pandas-1.4.2 pytz-2022.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "cb06e66d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import json \n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "sys.path.insert(0, 'usr/lib/chromium-browser/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "driver = webdriver.Chrome('chromedriver',options=options)\n",
    "# driver.set_window_size(1920, 1080)\n",
    "\n",
    "# print(r)\n",
    "\n",
    "datas = [];\n",
    "\n",
    "# recom_img_link = [];\n",
    "# disclaimer = [];\n",
    "# all_data_header = ['product_images', 'discount_percent', 'slider_images', 'medicine name', 'medicine type', \n",
    "#                 'manufacturer name', 'manufacturer link', 'special_price', 'regular_price', 'variation_option_name',\n",
    "#                'buy_btn_text', 'cart_btn_txt', 'product_description', 'recommended_products', \n",
    "#                 'recommended_products_link', 'disclaimer']\n",
    "# all_data_header = ['product_images', 'discount_percent']\n",
    "# all_data = np.vstack([all_data, all_data_header])\n",
    "url = [];\n",
    "url.append(\"https://osudpotro.com/humulin-30-70-cartridge-insulin\")\n",
    "url.append(\"https://osudpotro.com/respiromeer-romson-device\")\n",
    "\n",
    "for u in url: \n",
    "    all_data = dict();\n",
    "    product_images =[];\n",
    "    discount_percent = [];\n",
    "    slider_images = [];\n",
    "    title_name = [];\n",
    "    med_type = [];\n",
    "    a_reference = [];\n",
    "    a_text = [];\n",
    "    special_price_name = [];\n",
    "    special_price_val = [];\n",
    "    regular_price = [];\n",
    "    option_name = [];\n",
    "    product_description = [];\n",
    "    recommended_products = [];\n",
    "    reference_first_part = \"https://osudpotro.com/\";\n",
    "    recom_img_link = [];\n",
    "    driver.get(u)\n",
    "    r = driver.page_source\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    all_data['url'] = u\n",
    "    for img_div in soup.find_all('div',{'class': 'product-image-cont-compo'}):\n",
    "    ######################### product_images ###################\n",
    "        for im in img_div.find_all('img'):\n",
    "            product_images.append(im['src'])\n",
    "    #         data_one.append(product_images)\n",
    "            all_data['product_images'] = product_images\n",
    "#         print(all_data['product_images'])\n",
    "    # print(product_images)\n",
    "    ########################## discount_percent ####################\n",
    "        for dis in img_div.find_all('div',{'class':'discount-percent'}):\n",
    "            discount_percent.append(dis.text)\n",
    "    #         data_one.append(discount_percent)\n",
    "        \n",
    "        all_data['discount_percent'] = discount_percent\n",
    "    # print(discount_percent)\n",
    "    ######################### slider_images ###################\n",
    "    for sld in soup.find_all('div', {'class': 'image-swipper-cont'}):\n",
    "        for sld_img_div in sld.find_all('div', {'class': 'swiper-slide image-swipper swiper-slide-active'}):\n",
    "            for sld_im in sld_img_div.find_all('img'):\n",
    "                slider_images.append(sld_im['src'])\n",
    "            all_data['slider_images'] = slider_images\n",
    "    # print(slider_images)\n",
    "    for maindiv in soup.find_all('div', {'class': 'product-info-cont-compo product-details-details-inner-page col-lg-8'}):\n",
    "            for title_div in maindiv.find_all('div', {'class':'row'}):\n",
    "                for title_col in title_div.find_all('div', {'class':'col-lg-9'}):\n",
    "                    ######################### [h1] medicine name & type ###################\n",
    "                    for title in title_col.find_all('h1', { 'class' : 'product-info-heading-compo'}):                    \n",
    "                        for title_span in title.find_all('span', {'class' : 'sku'}):\n",
    "                            med_type.append(title_span.text)\n",
    "                            all_data['medicine type'] = med_type\n",
    "                        title.find('span').extract()\n",
    "                        title_name.append(title.getText().strip())\n",
    "                        all_data['medicine name'] = title_name\n",
    "#                         print( all_data['medicine name'])\n",
    "                    ######################### [p] manufacturer name, link ###################\n",
    "                    for p_manufacturer in title_col.find_all('p', {'class':'menufacturer'}):\n",
    "                        for manf_link in p_manufacturer.find_all('a', {'class' : 'nav-link'}):\n",
    "                            a_reference.append(reference_first_part+manf_link['href']);\n",
    "    #                         print( a_reference)\n",
    "                            a_text.append(manf_link.text)\n",
    "    #                         print(a_text)\n",
    "    \n",
    "                            all_data['manufacturer name'] = a_text\n",
    "                            all_data['manufacturer link'] = a_reference\n",
    "                    ######################### [label] prices ###################\n",
    "    #                 for price_label in title_col.find_all('label', {'class': 'product-price-info product-price'}):\n",
    "    #                     for bprice in price_label.find_all('span', {'class':'price-label'}):\n",
    "    # #                         print(bprice)\n",
    "    #                         special_price_name.append(bprice.text)\n",
    "    #                     for bprice_val in price_label.find_all('label', {'class' : 'price'}):\n",
    "    #                         special_price_val.append(bprice_val.text)\n",
    "    #                     for reg_price in price_label.find_all('span', {'class' : 'regular-price'}):\n",
    "    #                         regular_price.append(reg_price.text)\n",
    "    # print(regular_price)\n",
    "    data_price = json.loads(soup.find('script', id=\"__NEXT_DATA__\").text)\n",
    "#     print(data_price['props']['initialProps']['pageProps'])\n",
    "    inv = data_price['props']['initialProps']['pageProps']['productData']['inventory']\n",
    "    for prices in inv:\n",
    "        special_price_val.append(prices[\"price\"])\n",
    "        regular_price.append(prices[\"regular_price\"])\n",
    "        option_name.append(prices['variation_option_name'])\n",
    "        all_data['special_price'] = special_price_val\n",
    "        all_data['regular_price'] = regular_price\n",
    "        all_data['variation_option_name'] = option_name\n",
    "#         print(all_data['special_price'])\n",
    "#         print(all_data['regular_price'])\n",
    "#         print(all_data['variation_option_name'])\n",
    "    ######################### buttons ###################\n",
    "    for buttons in soup.find_all('div',{'class':'prod-control-buttons d-flex justify-content-between'}):\n",
    "        for buy_button in buttons.find_all('button', {'class':'btn buy-more-button ml-3 mr-2'}):\n",
    "            buy_button.find('i').extract()\n",
    "            buy_btn_text = buy_button.text\n",
    "    #         print(buy_btn_text)\n",
    "        all_data['buy_btn_text'] = buy_btn_text\n",
    "        for cart_butn in buttons.find_all('button', { 'class' : 'btn btn-secondary details-checkout-btn ml-2'}):\n",
    "            cart_butn.find('i').extract()\n",
    "            cart_btn_txt = cart_butn.text\n",
    "    #         print(cart_btn_txt)\n",
    "            \n",
    "        all_data['cart_btn_txt'] = cart_btn_txt\n",
    "    for pro_des in soup.find_all('div', { 'class' : 'd-none d-sm-block row'}):\n",
    "        product_description.append(pro_des)\n",
    "    all_data['product_description'] = product_description\n",
    "    # print(product_description)\n",
    "\n",
    "    for recom_row in soup.find_all('div', { 'class' : 'row'}):\n",
    "        for recom_col in recom_row.find_all('div', { 'class': 'col-12'}):\n",
    "            for recom_detail in recom_col.find_all('div', {'class': 'jss3'}):\n",
    "#                 recommended_products.append(recom_detail)            \n",
    "                for img_div in recom_detail.find_all('div', {'class':'swiper-container jss5 swiper-container-initialized swiper-container-horizontal swiper-container-pointer-events'}):\n",
    "                    for im in img_div.find_all('div', {'class':'jss6 swiper-slide product-item swiper-slide-active'}):\n",
    "                        recommended_products.append(im)      \n",
    "                        for img_a in im.find_all('a', {'class':'img-placeholder nav-link'}):                     \n",
    "    #                         recom_img_link.append(reference_first_part + img_a['href'])\n",
    "                            rimg_link = reference_first_part + img_a.get('href')\n",
    "                            recom_img_link.append(rimg_link)\n",
    "#                             print(recom_img_link)\n",
    "                        all_data['recommended_products_link'] = recom_img_link\n",
    "                        all_data['recommended_products'] = recommended_products\n",
    "#                 print(all_data['recommended_products'])\n",
    "    for disc_cont in soup.find_all('div' , { 'class': 'container'}):\n",
    "        for disc_row in disc_cont.find_all('div', {'class':'row'}):\n",
    "            for disc_col in disc_row.find_all('div', {'class':'col'}):\n",
    "                for disc_p in disc_col.find_all('p', {'class':'product-disclaimer'}):\n",
    "                    disclaimer.append(disc_p.text)   \n",
    "                all_data['disclaimer'] = disclaimer\n",
    "    datas.append(all_data)\n",
    "filename=\"osudhpotro_data.csv\"\n",
    "with open(filename, 'w', newline='') as f:\n",
    "    w=csv.DictWriter(f, [ 'url', 'product_images', 'discount_percent', 'slider_images', 'medicine type', 'medicine name', \n",
    "                          'manufacturer name', 'manufacturer link', 'special_price', 'regular_price', 'variation_option_name',\n",
    "                         'buy_btn_text', 'cart_btn_txt', 'product_description', 'recommended_products', \n",
    "                         'recommended_products_link', 'disclaimer'])\n",
    "    w.writeheader()\n",
    "    for all_data in datas:\n",
    "        w.writerow(all_data)\n",
    "# print(disclaimer)\n",
    "# from openpyxl import Workbook\n",
    "# filename = 'osudhpotro.xlxs'\n",
    "# workbook = Workbook()\n",
    "# sheet = workbook.active\n",
    "\n",
    "# print(all_data)\n",
    "# for i in all_data:  \n",
    "#     sheet.append(i)  \n",
    "# workbook.save('appending_values.xlsx')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "373fac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'discount': '3.50', 'item_variation_option_id': 13349, 'variation_option_id': 2, 'price': 463.2, 'regular_price': 480, 'variation_option_name': '1 Pc', 'qty_in_pcs': 1, 'stock_qty': 5, 'capacity': '1 Pc', 'in_cart': 0}, {'discount': '3.50', 'item_variation_option_id': 34870, 'variation_option_id': 104, 'price': 2316, 'regular_price': 2400, 'variation_option_name': '1 Box = 5 Pcs', 'qty_in_pcs': 5, 'stock_qty': 1, 'capacity': '1 Box = 5 Pcs', 'in_cart': 0}]\n",
      "463.2 \n",
      " 1 Pc\n",
      "2316 \n",
      " 1 Box = 5 Pcs\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "data_price = json.loads(soup.find('script', id=\"__NEXT_DATA__\").text)\n",
    "print(data['props']['initialProps']['pageProps']['productData']['inventory'])\n",
    "inv = data['props']['initialProps']['pageProps']['productData']['inventory']\n",
    "for item in inv:\n",
    "  print(item[\"price\"],'\\n',item['variation_option_name'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
